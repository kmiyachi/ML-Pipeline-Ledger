{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "/Users/kenmiyachi/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n  from numpy.core.umath_tests import inner1d\nINFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pandas_datareader import data as pdr\n",
    "import pandas as pd\n",
    "import fix_yahoo_finance as yf\n",
    "import importlib\n",
    "sys.path.append(\"../MachineLearningStocks/\")\n",
    "from download_historical_prices import *\n",
    "from parsing_keystats import *\n",
    "from stock_prediction import *\n",
    "sys.path.append(\"../awsQLDB/\")\n",
    "os.environ[\"AWS_PROFILE\"] = \"Insight\"\n",
    "from create_ledger import *\n",
    "from connect_to_ledger import *\n",
    "from create_table import *\n",
    "from create_index import * \n",
    "from insert_document import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[*********************100%***********************]  1 of 1 downloaded\n"
    }
   ],
   "source": [
    "data = build_sp500_dataset(\"2010-01-01\",\"2020-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectionData(dataPath, outHash):\n",
    "    repo = git.Repo(search_parent_directories=True)\n",
    "    commit_hash = repo.head.object.hexsha\n",
    "    collection_data = [\n",
    "        {\n",
    "            'GitHash': commit_hash,\n",
    "            'ScrapeTime': datetime.now(),\n",
    "            'DataPath': dataPath,\n",
    "            'outputHash': outHash\n",
    "        }\n",
    "    ]\n",
    "    return collection_data\n",
    "\n",
    "def trainingData(inputHash, model):\n",
    "    repo = git.Repo(search_parent_directories=True)\n",
    "    commit_hash = repo.head.object.hexsha\n",
    "    training_data = [\n",
    "        {\n",
    "            'GitHash': commit_hash,\n",
    "            'TrainTime': datetime.now(),\n",
    "            'InputHash': inputHash,\n",
    "            'Model': model\n",
    "        }\n",
    "    ]\n",
    "    return training_data\n",
    "\n",
    "def predictionData(inputHash, model, outHash):\n",
    "    repo = git.Repo(search_parent_directories=True)\n",
    "    commit_hash = repo.head.object.hexsha\n",
    "    prediction_data = [\n",
    "        {\n",
    "            'GitHash': commit_hash,\n",
    "            'InputHash': inputHash,\n",
    "            'Model': model, \n",
    "            'PredictionTime': datetime.now(),\n",
    "            'OutputHash': outHash\n",
    "        }\n",
    "    ]\n",
    "    return prediction_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collection():\n",
    "    build_sp500_dataset()\n",
    "    #execute_insert(\"Demo4Ledger\", \"Collection\", \"Test\", \"sp500_index.csv\")\n",
    "    data = collectionData(\"Purp\", \"New Hash.csv\")\n",
    "    dynamic_insert(\"Demo4Ledger\", \"Collection\", data)\n",
    "\n",
    "def feature_extraction(): \n",
    "    sp500_df, stock_df = preprocess_price_data()\n",
    "    parse_keystats(sp500_df, stock_df)\n",
    "    execute_insert(\"Demo4Ledger\", \"Training\", \"Hash(sp500_index.csv)\", \"Model1\")\n",
    "\n",
    "def predict():\n",
    "    predction = predict_stocks()\n",
    "    execute_insert(\"Demo4Ledger\", \"Prediction\", \"Hash(sp500_index.csv)\", \"Model1\", \"hash(prediction)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[*********************100%***********************]  1 of 1 downloaded\nINFO:insert_document:Inserting some documents in the Collection table...\nINFO:insert_document:Documents inserted successfully!\n"
    }
   ],
   "source": [
    "data_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:insert_document:Inserting some documents in the Collection table...\nINFO:insert_document:Documents inserted successfully!\n"
    }
   ],
   "source": [
    "    execute_insert(\"Demo4Ledger\", \"Collection\", \"KenJon\", \"sp500_index.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:insert_document:Inserting some documents in the Collection table...\nINFO:insert_document:Documents inserted successfully!\n"
    }
   ],
   "source": [
    "    execute_insert(\"Demo4Ledger\", \"Collection\", \"Test 1\", \"sp500_index.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "def randomString(stringLength=8):\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for i in range(stringLength))\n",
    "\n",
    "for _ in range(10):\n",
    "    execute_insert(\"Demo4Ledger\", \"Collection\", randomString(), \"streamTest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:insert_document:Inserting some documents in the Training table...\nINFO:insert_document:Documents inserted successfully!\nINFO:insert_document:Inserting some documents in the Prediction table...\nINFO:insert_document:Documents inserted successfully!\n"
    }
   ],
   "source": [
    "td = trainingData(\"THash\", \"Model1\")\n",
    "pd = predictionData(\"IOHash\", \"Model1\", \"PredictionData\")\n",
    "\n",
    "dynamic_insert(\"Demo4Ledger\", \"Training\", td)\n",
    "dynamic_insert(\"Demo4Ledger\", \"Prediction\", pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}